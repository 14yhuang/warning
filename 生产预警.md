

# 项目背景

当时的很多服务监控，是达不到秒级通知，基本都是分钟级别，但是对于一些场景出现的问题希望做到提前预知，这样可以做到提前调整规避问题出现，本项目主要是对平台日志进行监控，日志一般出自于mysql sql慢日志,nginx, tomcat, linux系统级别日志



# 架构

CDH HDFS服务 log --->flume--> kafka -->spark streaming 
-->influxdb(时序数据库) -->grafana



# 需求

**flume采集的原始日志**

日志格式log4j:
时间 日志级别 日志详细内容 

比如采集/opt/datas/hadoop-cmf-hdfs-NAMENODE-yws76.log.out的日志格式为

```
{"time":"2018-03-19 09:58:39,093","logtype":"ERROR","loginfo":"org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:StorageInfo TreeSet fill ratio DS-ae1d00cb-4f74-45bf-a44c-7aab59ceb896 : 1.0"}
```



对于日志格式需求是，希望给每个日志增加两列信息，机器名称 和服务名称，所以需要二次开发execsource



**改造后的日志**

json日志：

机器名称 服务名称 时间 日志级别 日志详细内容



# 1.HDFS日志格式改为json

 把这个配置为json格式
log4j.appender.RFA.layout.ConversionPattern = {"time":"%d{yyyy-MM-dd HH:mm:ss,SSS}","logtype":"%p","loginfo":"%c:%m"}%n



![img](https://img-blog.csdnimg.cn/20190221083022683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NDU5Mzg2,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190221083052143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NDU5Mzg2,size_16,color_FFFFFF,t_70)







# 2.flume源码改造

开发思路：
  直接基于源代码中/Users/apple/Desktop/ruoze/flume-1.7.0/flume-ng-core/src/main/java/org/apache/flume/source/ExecSource.java进行修改，重写run方法


```
package com.onlinelog.analysis;

/*
 * 
 * 
 * 主要修改代码：
 * 1.在每一行日志(json格式)的前部加上机器名称 和 服务名称
 * 2.机器名称和服务名称从配置文件中读取，假如没有指定，就使用默认
 * 
 * json:
 * {
    "time": "2018-05-23 14:00:46,641",
    "logtype": "INFO",
    "loginfo": "org.apache.catalina.core.AprLifecycleListener:The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: /usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib"
	}
 * 
 */


import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ScheduledFuture;
import java.util.concurrent.TimeUnit;

//import org.apache.flume.Channel;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.EventDrivenSource;
//import org.apache.flume.Source;
import org.apache.flume.SystemClock;
import org.apache.flume.channel.ChannelProcessor;
import org.apache.flume.conf.Configurable;
import org.apache.flume.event.EventBuilder;
import org.apache.flume.instrumentation.SourceCounter;
import org.apache.flume.source.AbstractSource;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Preconditions;
import com.google.common.util.concurrent.ThreadFactoryBuilder;

import java.nio.charset.Charset;


public class ExecSource_JSON extends AbstractSource implements EventDrivenSource, Configurable {

  private static final Logger logger = LoggerFactory.getLogger(ExecSource_JSON.class);

  private String shell;
  private String command;
  private SourceCounter sourceCounter;
  private ExecutorService executor;
  private Future<?> runnerFuture;
  private long restartThrottle;
  private boolean restart;
  private boolean logStderr;
  private Integer bufferCount;
  private long batchTimeout;
  private ExecRunnable runner;
  private Charset charset;
  
  //机器名称  服务名称
  private String hostname;
  private String servicename;
  
  @Override
  public void start() {
    logger.info("Exec source starting with command:{}", command);

    executor = Executors.newSingleThreadExecutor();

    runner = new ExecRunnable(shell, command, getChannelProcessor(), sourceCounter,
        restart, restartThrottle, logStderr, bufferCount, batchTimeout, charset,hostname,servicename);

    // FIXME: Use a callback-like executor / future to signal us upon failure.
    runnerFuture = executor.submit(runner);

    /*
     * NB: This comes at the end rather than the beginning of the method because
     * it sets our state to running. We want to make sure the executor is alive
     * and well first.
     */
    sourceCounter.start();
    super.start();

    logger.debug("Exec source started");
  }

  @Override
  public void stop() {
    logger.info("Stopping exec source with command:{}", command);
    if (runner != null) {
      runner.setRestart(false);
      runner.kill();
    }

    if (runnerFuture != null) {
      logger.debug("Stopping exec runner");
      runnerFuture.cancel(true);
      logger.debug("Exec runner stopped");
    }
    executor.shutdown();

    while (!executor.isTerminated()) {
      logger.debug("Waiting for exec executor service to stop");
      try {
        executor.awaitTermination(500, TimeUnit.MILLISECONDS);
      } catch (InterruptedException e) {
        logger.debug("Interrupted while waiting for exec executor service "
            + "to stop. Just exiting.");
        Thread.currentThread().interrupt();
      }
    }

    sourceCounter.stop();
    super.stop();

    logger.debug("Exec source with command:{} stopped. Metrics:{}", command,
        sourceCounter);
  }

  @Override
  public void configure(Context context) {
    command = context.getString("command");

    Preconditions.checkState(command != null,
        "The parameter command must be specified");

    restartThrottle = context.getLong(ExecSourceConfigurationConstants.CONFIG_RESTART_THROTTLE,
        ExecSourceConfigurationConstants.DEFAULT_RESTART_THROTTLE);

    restart = context.getBoolean(ExecSourceConfigurationConstants.CONFIG_RESTART,
        ExecSourceConfigurationConstants.DEFAULT_RESTART);

    logStderr = context.getBoolean(ExecSourceConfigurationConstants.CONFIG_LOG_STDERR,
        ExecSourceConfigurationConstants.DEFAULT_LOG_STDERR);

    bufferCount = context.getInteger(ExecSourceConfigurationConstants.CONFIG_BATCH_SIZE,
        ExecSourceConfigurationConstants.DEFAULT_BATCH_SIZE);

    batchTimeout = context.getLong(ExecSourceConfigurationConstants.CONFIG_BATCH_TIME_OUT,
        ExecSourceConfigurationConstants.DEFAULT_BATCH_TIME_OUT);

    charset = Charset.forName(context.getString(ExecSourceConfigurationConstants.CHARSET,
        ExecSourceConfigurationConstants.DEFAULT_CHARSET));

    shell = context.getString(ExecSourceConfigurationConstants.CONFIG_SHELL, null);
    
    // 在此方法中，通过context.getString("property", defaultValue)  
    // 读取参数配置  ,没有指定就默认配置
    hostname= context.getString("hostname","xxx");
    servicename=context.getString("servicename", "xxx");
    
    if (sourceCounter == null) {
      sourceCounter = new SourceCounter(getName());
    }
  }

  private static class ExecRunnable implements Runnable {

    public ExecRunnable(String shell, String command, ChannelProcessor channelProcessor,
        SourceCounter sourceCounter, boolean restart, long restartThrottle,
        boolean logStderr, int bufferCount, long batchTimeout, Charset charset,String hostname,String servicename) {
      this.command = command;
      this.channelProcessor = channelProcessor;
      this.sourceCounter = sourceCounter;
      this.restartThrottle = restartThrottle;
      this.bufferCount = bufferCount;
      this.batchTimeout = batchTimeout;
      this.restart = restart;
      this.logStderr = logStderr;
      this.charset = charset;
      this.shell = shell;
      this.hostname=hostname;
      this.servicename=servicename;
      
    }

    private final String shell;
    private final String command;
    private final ChannelProcessor channelProcessor;
    private final SourceCounter sourceCounter;
    private volatile boolean restart;
    private final long restartThrottle;
    private final int bufferCount;
    private long batchTimeout;
    private final boolean logStderr;
    private final Charset charset;
    private Process process = null;
    private SystemClock systemClock = new SystemClock();
    private Long lastPushToChannel = systemClock.currentTimeMillis();
    ScheduledExecutorService timedFlushService;
    ScheduledFuture<?> future;
    
    //机器名称  服务名称
    private String hostname;
    private String servicename;
    
    //临时event  临时剩余行
    private String tmpevent="";
    private String tmpremainlines="";
    private String[] linespilt=null;
    //jsonvalidator对象
    JsonValidator jsonvalidator = new JsonValidator() ;
    
    @Override
    public void run() {
      do {
        String exitCode = "unknown";
        BufferedReader reader = null;
        String line = null;
        final List<Event> eventList = new ArrayList<Event>();

        timedFlushService = Executors.newSingleThreadScheduledExecutor(
                new ThreadFactoryBuilder().setNameFormat(
                "timedFlushExecService" +
                Thread.currentThread().getId() + "-%d").build());
        try {
          if (shell != null) {
            String[] commandArgs = formulateShellCommand(shell, command);
            process = Runtime.getRuntime().exec(commandArgs);
          }  else {
            String[] commandArgs = command.split("\\s+");
            process = new ProcessBuilder(commandArgs).start();
          }
          reader = new BufferedReader(
              new InputStreamReader(process.getInputStream(), charset));

          // StderrLogger dies as soon as the input stream is invalid
          StderrReader stderrReader = new StderrReader(new BufferedReader(
              new InputStreamReader(process.getErrorStream(), charset)), logStderr);
          stderrReader.setName("StderrReader-[" + command + "]");
          stderrReader.setDaemon(true);
          stderrReader.start();

          future = timedFlushService.scheduleWithFixedDelay(new Runnable() {
              @Override
              public void run() {
                try {
                  synchronized (eventList) {
                    if (!eventList.isEmpty() && timeout()) {
                      flushEventBatch(eventList);
                    }
                  }
                } catch (Exception e) {
                  logger.error("Exception occured when processing event batch", e);
                  if (e instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                  }
                }
              }
          },
          batchTimeout, batchTimeout, TimeUnit.MILLISECONDS);

          
          while ((line = reader.readLine()) != null) {
        	  
            synchronized (eventList) {
            	
              sourceCounter.incrementEventReceivedCount();
              
              
              try{
            	  
            	  //判断line是含有日志级别的标志符
                  if(line.contains("INFO")==true || line.contains("WARN")==true || line.contains("ERROR")==true || line.contains("DEBUG")==true || line.contains("FATAL")){
                	  
                	  //假如有，就将上一个的event的add到eventlist
                	  if(tmpevent.length()>0){
                		  
                		  if(tmpremainlines.length()>0){ 
                			  /*
                			   * 判断多行时会出现两种
                			   * a.当采集cdh的hdfs服务时,  "} 出现在最后一行结尾
							{"time":"2018-05-28 17:14:03,490","logtype":"DEBUG","loginfo":"org.apache.hadoop.hdfs.server.datanode.DataNode:Receiving one packet for block BP-116669189-192.168.1.110-1448769719691:blk_796610962946870242_1563368: PacketHeader with packetLen=0 header data: offsetInBlock: 56
							seqno: 1
							lastPacketInBlock: true
							dataLen: 0
							"}
								
                			   * b.当采集tomcat服务时,  "} 出现在第一行结尾
							{"time":"2018-05-28 16:43:07,287","logtype":"INFO","loginfo":"org.apache.catalina.startup.Catalina:Server startup in 10589 ms"}
							{"time":"2018-05-28 16:43:07,290","logtype":"ERROR","loginfo":"org.apache.catalina.core.StandardServer:StandardServer.await: create[localhost:8005]: "}
							java.net.BindException: Address already in use
							        at java.net.PlainSocketImpl.socketBind(Native Method)
							        at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:322)
							        at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:456)
							{"time":"2018-05-28 16:43:07,292","logtype":"INFO","loginfo":"org.apache.coyote.http11.Http11Protocol:Pausing ProtocolHandler ["http-bio-8080"]"}
                			   
                			   
                			   * c.
                			   *  {"time":"2018-05-12 16:16:02,481","logtype":"INFO","loginfo":"org.apache.hadoop.hdfs.server.datanode.DataNode:SHUTDOWN_MSG: 
							   *  /************************************************************
							   *  SHUTDOWN_MSG: Shutting down DataNode at sht-sgmhadoopdn-02/172.16.101.59
							   *  ************************************************************'/"}
                			   */
                			  
                			  
                			  if(tmpevent.indexOf("\"}")>0){  //b
                				  
                				  tmpevent=tmpevent.replaceAll("\"}", tmpremainlines+"\"}");
                				  
                			
                			  }else{//a,c
                				  // 将最后一个字符} 换成 "}
                				  tmpevent=tmpevent+tmpremainlines.substring(0,tmpremainlines.length()-1)+"\"}";
                			  }
                			 
                			  
                		  }
                		  
                		  //验证tmpevent是否为json格式  不验证的话 很有可能非json格式数据进入后面的hive spark计算,会导致job失败
                		  if(jsonvalidator.validate(tmpevent)){
                			  eventList.add(EventBuilder.withBody(tmpevent.getBytes(charset))); 
                		  }
                		  tmpevent=""; 
                		  tmpremainlines="";
                	  }
                      
                	  //先处理loginfo的value的字符串
                	 
                	   linespilt=line.split("\"loginfo\":");
                	   line=linespilt[0]+"\"loginfo\":"+"\""
                	   +linespilt[1].replaceAll("(\r\n|\r|\n|\n\r|\"|\b|\f|\\|/)","")
                	   .replaceAll("(\t)", "    ")
                	   .replaceAll("(\\$)", "@")
                	   .replaceAll("}","\"}");
                	   
                	  //将当前行赋值给对象tmpline，等待下一个含有日志级别的标志符的行时，才会提交到eventlist
                	  tmpevent=new StringBuffer(line).insert(1,"\"hostname\":\""+hostname+"\"," + "\"servicename\":\""+servicename+"\",").toString();
                	
                	
                    
                  }else if(line == null || "".trim().equals(line)) { //判断是否为空白行，则结束本次循环，继续下一次循环
                       continue;
                       
                  }else{  
                	  //判断line不包含有日志级别的标志符,去除换行符 回车符 双引号 制表符，
                	  //使用<@@@>拼接
                	  line=line.replaceAll("(\r\n|\r|\n|\n\r|\"|\b|\f|\\|/)", "")
                			  .replaceAll("(\t)", "    ")
                			  .replaceAll("(\\$)", "@");
                			  

                	  tmpremainlines=tmpremainlines+"<@@@>"+line;
                	  
                  }
                  
            	  
              }catch(Exception ex){
            	  logger.error("log handle exception: ", ex);
            	  tmpevent=""; 
        		  tmpremainlines="";
            	  continue;
              }
             
             
              
              //满足bufferCount则 将eventlist flush推送到chnnel
              if (eventList.size() >= bufferCount || timeout()) {
                flushEventBatch(eventList);
              }
              
            }
            
          }
          
          
          

          synchronized (eventList) {
            if (!eventList.isEmpty()) {
              flushEventBatch(eventList);
            }
          }
        } catch (Exception e) {
          logger.error("Failed while running command: " + command, e);
          if (e instanceof InterruptedException) {
            Thread.currentThread().interrupt();
          }
        } finally {
          if (reader != null) {
            try {
              reader.close();
            } catch (IOException ex) {
              logger.error("Failed to close reader for exec source", ex);
            }
          }
          exitCode = String.valueOf(kill());
        }
        if (restart) {
          logger.info("Restarting in {}ms, exit code {}", restartThrottle,
              exitCode);
          try {
            Thread.sleep(restartThrottle);
          } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
          }
        } else {
          logger.info("Command [" + command + "] exited with " + exitCode);
        }
      } while (restart);
    }

    private void flushEventBatch(List<Event> eventList) {
      channelProcessor.processEventBatch(eventList);
      sourceCounter.addToEventAcceptedCount(eventList.size());
      eventList.clear();
      lastPushToChannel = systemClock.currentTimeMillis();
    }

    private boolean timeout() {
      return (systemClock.currentTimeMillis() - lastPushToChannel) >= batchTimeout;
    }

    private static String[] formulateShellCommand(String shell, String command) {
      String[] shellArgs = shell.split("\\s+");
      String[] result = new String[shellArgs.length + 1];
      System.arraycopy(shellArgs, 0, result, 0, shellArgs.length);
      result[shellArgs.length] = command;
      return result;
    }

    public int kill() {
      if (process != null) {
        synchronized (process) {
          process.destroy();

          try {
            int exitValue = process.waitFor();

            // Stop the Thread that flushes periodically
            if (future != null) {
              future.cancel(true);
            }

            if (timedFlushService != null) {
              timedFlushService.shutdown();
              while (!timedFlushService.isTerminated()) {
                try {
                  timedFlushService.awaitTermination(500, TimeUnit.MILLISECONDS);
                } catch (InterruptedException e) {
                  logger.debug("Interrupted while waiting for exec executor service "
                      + "to stop. Just exiting.");
                  Thread.currentThread().interrupt();
                }
              }
            }
            return exitValue;
          } catch (InterruptedException ex) {
            Thread.currentThread().interrupt();
          }
        }
        return Integer.MIN_VALUE;
      }
      return Integer.MIN_VALUE / 2;
    }
    public void setRestart(boolean restart) {
      this.restart = restart;
    }
  }

  private static class StderrReader extends Thread {

    private BufferedReader input;
    private boolean logStderr;

    protected StderrReader(BufferedReader input, boolean logStderr) {
      this.input = input;
      this.logStderr = logStderr;
    }

    @Override
    public void run() {
      try {
        int i = 0;
        String line = null;
        while ((line = input.readLine()) != null) {
          if (logStderr) {
            // There is no need to read 'line' with a charset
            // as we do not to propagate it.
            // It is in UTF-16 and would be printed in UTF-8 format.
            logger.info("StderrLogger[{}] = '{}'", ++i, line);
          }
        }
      } catch (IOException e) {
        logger.info("StderrLogger exiting", e);
      } finally {
        try {
          if (input != null) {
            input.close();
          }
        } catch (IOException ex) {
          logger.error("Failed to close stderr reader for exec source", ex);
        }
      }
    }
  }
}

```





# 3.flume配置

exec-memory-kafka

```
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the custom exec source
a1.sources.r1.type = com.onlinelog.analysis.ExecSource_JSON
a1.sources.r1.command = tail -F /opt/datas/hadoop-cmf-hdfs-NAMENODE-yws76.log.out
a1.sources.r1.hostname = bigdata-pro01.kfk.com
a1.sources.r1.servicename = namenode

# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = DSHS
a1.sinks.k1.kafka.bootstrap.servers = bigdata-pro01.kfk.com:9092,bigdata-pro02.kfk.com:9092,bigdata-pro03.kfk.com:9092
a1.sinks.k1.kafka.flumeBatchSize = 6000
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.ki.kafka.producer.compression.type = snappy

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.keep-alive = 90
a1.channels.c1.capacity = 2000000
a1.channels.c1.transactionCapacity = 6000

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```



# 4.kafka

创建kafka topic

```
kafka-topics.sh --create --zookeeper bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181 --replication-factor 3 --partitions 3 --topic DSHS
```
flume写入kafka消费：

```
afka-console-consumer.sh --zookeeper bigdata-pro01.kfk.com:2181,bigdata-pro02.kfk.com:2181,bigdata-pro03.kfk.com:2181 --topic DSHS --from-beginning
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190211224602397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Y3Vvc2hpODcxOA==,size_16,color_FFFFFF,t_70)



# 5.sparkstreaming写到influxdb

```
import java.util

import com.java.main.InfluxDBUtils
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.{Duration, Seconds, StreamingContext}
import org.influxdb.{InfluxDB, InfluxDBFactory}
import org.json.JSONObject

object OnlineLogAnalysisAlert2 {

  // var influxDB: InfluxDB = _
  //定义连接influxdb
  val influxDB = InfluxDBFactory.connect("http://" + InfluxDBUtils.getInfluxIP + ":" + InfluxDBUtils.getInfluxPORT(true), "admin", "admin")
  val rp = InfluxDBUtils.defaultRetentionPolicy(influxDB.version)

  def main(args: Array[String]): Unit = {
    System.setProperty("hadoop.home.dir", "D:\\hadoop")
	logAnalysis
  }

  def logAnalysis = {
  var bcAlertList: List[_] = null

//定义连接influxdb
val influxDB = InfluxDBFactory.connect("http://" + InfluxDBUtils.getInfluxIP + ":" + InfluxDBUtils.getInfluxPORT(true), "admin", "admin")
val rp = InfluxDBUtils.defaultRetentionPolicy(influxDB.version)

val dbName = "online_log_analysis"
// 准备工作
val spark = new SparkSession.Builder().master("local[2]").appName("OnlineLogAnalysis").getOrCreate()

val sc = spark.sparkContext
val ssc = new StreamingContext(sc, Seconds(10))
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "hadoop000:9092",
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "use_a_separate_group_id_for_each_stream",
  "auto.offset.reset" -> "latest",
  "enable.auto.commit" -> (false: java.lang.Boolean)
)

val topics = Array("onlinelogs")
val dStream = KafkaUtils.createDirectStream[String, String](
  ssc,
  PreferConsistent,
  Subscribe[String, String](topics, kafkaParams)
)

val logline = dStream.map(record => (record.key, record.value))
// logline.filter(logline.value.contains("INFO") == true || logline.value.contains("WARN") == true || logline.value.contains("ERROR") == true || logline.value.contains("DEBUG") == true || logline.value.contains("FATAL") == true)
val logInfo = logline.filter(x => x._2.contains("INFO") || x._2.contains("WARN") || x._2.contains("ERROR") || x._2.contains("DEBUG") || x._2.contains("FATAL"))
  .map(_._2)
//  .print()
//RDD转为DF
//转换为json格式
var cdhRoleLog = null
//TODO... 通过读取解析得到schema
val schema = StructType(
  Array(
    StructField("hostName", StringType, true),
    StructField("serviceName", StringType, false),
    StructField("logType", StringType, false),
    StructField("lineTimestamp", StringType, false),
    StructField("logInfo", StringType, false)))
//或 val schema = "b float, c short"
val infoRdd = logInfo.map(x => {
  val jsonlogline = new JSONObject(x)
  Row(jsonlogline.getString("hostname"),
    jsonlogline.getString("servicename"),
    jsonlogline.getString("logtype"),
    jsonlogline.getString("time"),
    jsonlogline.getString("loginfo")
  )
})
val windowDStream = infoRdd.window(Seconds(10), new Duration(10 * 1000))

var cdhRoleLogDR: DataFrame = null
windowDStream.foreachRDD(cdhRoleLogRDD => {
  //判断rdd的数目
  if (cdhRoleLogRDD.count == 0) {
    System.out.println("No cdh role logs in this time interval")

  } else {

    // 8.2从RDD创建Dataset
    cdhRoleLogDR = spark.createDataFrame(cdhRoleLogRDD, schema)

    // cdhRoleLogDR.show()

    //8.3注册为临时表
    cdhRoleLogDR.createOrReplaceTempView("cdhrolelogs")

    //8.4调用自定义alert广播变量

    bcAlertList = BroadcastAlert.getInstance.updateAndGet(spark, bcAlertList)
    val alertInfoList = bcAlertList

    //8.5拼接SQL
    var alertsql = ""
    var sqlstr = ""
    if (alertInfoList != null && alertInfoList.size > 0) { //定义alertsql
      System.out.println("print custom alert words:")
      for (alertInfo <- alertInfoList) {
        System.out.println(alertInfo)
        val stralertInfo = alertInfo.toString
        val alertInfostr = stralertInfo.substring(stralertInfo.indexOf("[") + 1, stralertInfo.lastIndexOf("]"))
        alertsql = alertsql + " logInfo like '%" + alertInfostr + "%' or"
      }
      alertsql = alertsql.substring(1, alertsql.length - 2)
      //定义sql
      sqlstr = "SELECT hostName,serviceName,logType,COUNT(logType) FROM cdhrolelogs GROUP BY hostName,serviceName,logType union all " +
        "SELECT t.hostName,t.serviceName,t.logType,COUNT(t.logType) FROM " + "(SELECT hostName,serviceName,'alert' logType FROM cdhrolelogs where " + alertsql + ") t " + " GROUP BY t.hostName,t.serviceName,t.logType"
    } else {
      sqlstr = "SELECT hostName,serviceName,logType,COUNT(logType) FROM cdhrolelogs GROUP BY hostName,serviceName,logType"
    }
    //8.6计算结果为List<Row>
    val logtypecount = spark.sql(sqlstr).collectAsList()
    cdhRoleLogDR.show()
    var value = ""
    //8.7循环处理
    import scala.collection.JavaConversions._
    var host_service_logtype: String = ""
    for (rowlog <- logtypecount) {
      host_service_logtype = rowlog.get(0) + "_" + rowlog.get(1) + "_" + rowlog.get(2)
      value = value + "logtype_count,host_service_logtype=" +
        host_service_logtype + " count=" + String.valueOf(rowlog.getLong(3)) + "\n"
    }
    //8.8 存储至influxdb
    if (value.length > 0) {
      value = value.substring(0, value.length) //去除最后一个字符“,”

      //打印
      System.out.println(value);
      //保存
      influxDB.write(dbName, rp, InfluxDB.ConsistencyLevel.ONE, value)
    }
  }

})
ssc.start()
ssc.awaitTermination()
 }

}
```

# 6.项目演示结果

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190212163546595.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Y3Vvc2hpODcxOA==,size_16,color_FFFFFF,t_70)



# influxdb(时序数据库)

- **influxdb(时序数据库)**
  用yum安装influxdb-1.7.0.x86_64.rpm(离线下载)
  service influxdb start ==启动==
  默认端口：8086
  influx -precision rfc3339 ==进入influxdb的shell终端==
  根据项目需求创建数据库  create database online_log_analysis

  ```
  <measurement>[,<tag-key>=<tag-value>...] <field-key>=<field-value>[,<field2-key>=<field2-value>...] [unix-nano-timestamp]
  表名称        主键1=主键值1,主键2=主键值2,  值key1=值value1,值key2=值value2
  stu            id                            name , age
  ```
  influxdb中表为measurement，==注意：值必须为数值型==
  ```
  INSERT cpu,host=serverA,region=us_west value=0.64,value1=0.8
  ```


  ```
  参考文档：https://docs.influxdata.com/influxdb/v1.7/query_language/database_management/
  ```

# Grafana

用yum install grafana-5.2.1-1.x86_64.rpm安装
service grafana-server start 启动

配置数据源：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190212161845348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Y3Vvc2hpODcxOA==,size_16,color_FFFFFF,t_70)
新建dashboard：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190212161934236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Y3Vvc2hpODcxOA==,size_16,color_FFFFFF,t_70)



# 总结

主要对HDFS服务和Tomcat服务进行实时计算预警，故要做以下加强功能:

1. 改造 CDH/Apache hadoop的hdfs的nn,dn进程的日志输出格式,每一条改为json格式输出 (之前每一条为 string)
2. 改造 Tomcat的日志支持log4j,其为json格式输出

3. 改造 基于Flume-ng Exec Source开发自定义插件ExecSource_JSON,
  支持JSON数据处理(日志折断)

4. 改造 spark streaming+spark sql,支持读取json解析

5. 使用添加到MySQL表中,同时支持添加自定义监控词
6. 改造 spark streaming+spark sql,参数配置定时读取监控词库,进行预警计算
7. 改造grafana dashboard可视化
8. 整个架构高可靠设计和优化 
9. 扩展其他应用、DB的日志等等

